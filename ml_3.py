# -*- coding: utf-8 -*-
"""ML_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10_B3VVK-c0HIZBxgYNmaJxv0MVBWAWjO
"""

import numpy as np
import matplotlib.pyplot as plt

# Take user input for coefficients of quadratic ax^2 + bx + c
a = float(input("Enter coefficient a: "))
b = float(input("Enter coefficient b: "))
c = float(input("Enter coefficient c: "))
x0 = float(input("Enter starting x: "))
lr = float(input("Enter learning rate: "))
iters = int(input("Enter number of iterations: "))

def f(x):
  return a*x**2 + b*x + c

def grad(x):
  return 2*a*x + b

x=x0
path = [x]
for _ in range(iters):
  x = x - lr*grad(x)
  path.append(x)

x_plot = np.linspace(x0-10,x0+10,100)
y_plot = f(x_plot)
plt.plot(x_plot, y_plot, label='f(x)')
plt.plot(path, f(np.array(path)),'ro-',label='GD Path')
plt.title('Gradient Descent Algorithm')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid(True)
plt.show()

print('Local minimum at: x=',path[-1],' y=',f(path[-1]))

# import matplotlib.pyplot as plt
# import numpy as np
# def func1(x):
#   return (x+3)**2

# def gradient_func1(x):
#    return 2*(x+3)

# def gradient_descent(function, start, learn_rate, n_iter = 100, tolerance = 0.1):
#     gradient = gradient_func1
#     function = func1
#     points = [start]
#     iters = 0

#     while iters < n_iter:
#         prev_x = start
#         start = start - learn_rate * gradient(prev_x)
#         iters = iters+1
#         points.append(start)
#     print("The local minimum occurs at", start)

#     x_ = np.linspace(-7, 5, 100)
#     y = function(x_)

#     fig = plt.figure(figsize = (10, 10))
#     plt.plot(x_, y, 'g')
#     plt.plot(points, function(np.array(points)), '-o')

#     plt.show()

# gradient_descent(function = func1, start = 2.0, learn_rate = 0.2, n_iter = 50)